---
header-includes:
output: pdf_document
---

\definecolor{greenJoe}{RGB}{0,128,128}
\definecolor{blueJoe}{RGB}{0,0,250}
\definecolor{redJoe}{RGB}{200,0,0}


\begin{center}
\huge{ \textcolor{blueJoe}{\textbf{Final MovieLens Report}} }

\large{\textcolor{greenJoe}{\textbf{Ing. José Ramón Riesgo Escovar}} }

May 2020
\end{center}


\begin{center}

\Large { \textcolor{blue}{Introduction to the Report} }

\end{center}

This report is part of the Capstone project for the course \textcolor{greenJoe}{\textbf{HarvardX: PH125.9x Data Science: Capstone}} the goal is to demonstrate the acquire skills within R in the field of Data Science, in order to solve real world problems.

The goal is to generate a recommendation model that is able to predict the "rating" and the "potential" evaluation a user will give to a specific movie.

Classification Systems are based on analyzing the actual ratings previous users made to the movies, allowing them to predict their future rating of others. 

In October 2006, Netflix offered to the data science community a challenge where anyone that can improve the recommendation algorithm by 10% will win a million dollars; on September 2009, the winner was announced. Taking into consideration the data analysis strategies used by the winners, this paper will propose a model based on a similar algorithm which generates movies´ratings.

Due that the actual information of Netflix is not available, we will use the information from MovieLens. This company was created by GroupLens Research, responsable of building data sets from the movies that were collected over various periods of time; for this specific study, the sample will be 10M data set.

We are going to load all the information, first the 90% of it will be used to generate a training set called \textcolor{blueJoe}{\textbf{edx}}. The remaining 10% will be defined as the final testing set, known as \textcolor{blueJoe}{\textbf{validation}}; which later on will be execute against the training group to define the "best" model.

```{r Load Libraries, message=FALSE, warning=FALSE, include=FALSE}
#LOADING ALL LIBRARIES WE WILL BE USING
library(dplyr)
library(ggplot2)
library(tidyverse)
library(markdown)
library(caret)
library(knitr)
```

As said, the first is to upload the \textcolor{blueJoe}{\textbf{edx}} information, generating the following summary:

```{r Load edx, echo=FALSE, fig.align='center', fig.height=4, fig.width=4}

load(file = "rda/edx.rda")
summary(edx)
```


As seen in the table, the average rating "stars" is \textcolor{blueJoe}{\textbf{3.512}}; showing a minimum of .5 and maximum of 5; being this range the one consider in the propose model.

The objective of the model is that the ratings generated by the "best" produce algorithm against the \textcolor{blueJoe}{\textbf{validation}} data, represents a root-mean-square-error (RMSE) smaller than < \textcolor{blueJoe}{\textbf{.86490}}.

For the calculation of the RMSE, the formula will be:

 
$$\sqrt{ \frac{1}{N} \sum_{u, i} ( \hat{y}_{u, i} - y_{u, i} )^2}$$


Meaning that in R language, the code for doing so is:


```{r RMSE Function, echo=TRUE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The next step is the division of the data set \textcolor{blueJoe}{\textbf{edx}} in a training and test groups with the following characteristics:
- Setting the seed to 1, so always you have the same result
- Dividing the \textcolor{blueJoe}{\textbf{edx}} data, where 80% is considered training and 20% test set.
- Defining the training set as \textcolor{greenJoe}{\textbf{train\_set}} and test set as \textcolor{greenJoe}{\textbf{test\_set}} with the following code:

```{r Split edx, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```

Before running the calculation, it is necessary to revise there are no users, movies, and genres in the test group that are not present in the training set. For doing so, the following code should be use:

```{r Verify Test Set, echo=TRUE}
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId") %>%
  semi_join(train_set, by = "genres")
```

\begin{center}

\Large { \textcolor{blue}{Methods and Analysis:} }

\end{center}

In this section, there will be shown several models; starting with the most general one and concluding with a more precise, meaning this last one is the "best" possible model to achieve the goal mention above.

\begin{center}

\large { \textcolor{blueJoe}{\textbf{FIRST MODEL}} }

\end{center}

For the first and more simple model, there will be use the average values of the training set with the following formula:

$$Y_{u, i} = \mu + \epsilon_{u, i}$$

Assuming same ratings for all movies and all users.

Calculating it with the following code:

```{r Calculate Mean, echo=TRUE}
mu_adv <- mean(train_set$rating)
```

Within the variable \textcolor{greenJoe}{\textbf{mu\_adv}} there will be kept the mean of the ratings.

The value of \textcolor{greenJoe}{\textbf{mu\_adv}} is \textcolor{redJoe}{\textbf{3.5124}}

Executing the RMSE function and comparing the generated \textcolor{greenJoe}{\textbf{mu\_adv}} with the actual ratings of the test set:

```{r Results First Algorithm, echo=TRUE}
mu_model <- RMSE( mu_adv,test_set$rating)
```

The actual result of the RMSE of the first Algorithm is: \textcolor{redJoe}{\textbf{1.0599}}

The results of each model are captured in the table of "results of the RMSE" of each model, named the table \textcolor{greenJoe}{$\textbf{rmse\_results}$}

```{r Results First Entry, echo=TRUE}
rmse_results <- data.frame(METHOD = "Simple Average Model", RMSE = mu_model )
```

The result of the first algorithm is:

```{r Show Table part1, echo=FALSE, fig.height=3, fig.width=3}
rmse_results %>% knitr::kable()
```

\begin{center}

\large { \textcolor{blueJoe}{\textbf{SECOND MODEL}} }

\end{center}

The first model was very basic, now exploring more the data with a chart, it is shown that some movies get higher ratings than others:

```{r Generate Chart, echo=FALSE, fig.height=3, fig.width=6, fig.align='center'}
test_set %>% 
group_by(movieId) %>%
summarize(howmany = n(), average = mean(rating)) %>%
arrange(desc(howmany)) %>%
ggplot(aes(average,howmany)) +
geom_point(colour = "light green", size = 1) +
theme_light() +
labs(title = " COMPARING EVALUATIONS AGAINST THEIR AVERAGE RATINGS") +
xlab("MEAN OF RATINGS") + 
ylab("NUMBER OF EVALUATIONS") + 
theme(plot.title = element_text(color = "blue", size = 9, 
                                  face = "bold", hjust = 0.5)) +
theme(axis.title.x = element_text(color = "grey", size = 8, 
                                    face = "bold", hjust = 0.5)) +
theme(axis.title.y = element_text(color = "grey", size = 8, 
                                    face = "bold", hjust = 0.5)) +
theme(axis.text.x = element_text(color = "light blue", size = 8, 
                                    face = "bold", hjust = 0.5)) +
theme(axis.text.y = element_text(color = "light blue", size = 8, 
                                   face = "bold", hjust = 0.5)) +
annotate("rect", xmin=2.2, xmax=4.5, ymin=500, ymax=6500, alpha=0.2, 
           fill="light yellow", color= "black")
```

There is a clear tendency highlighted in a rectangle, meaning that a higher number of evaluations tend to get a higher average in the ratings

Based on this fact, the previous model can be augment by adding an average ranting per movie; transforming the formula to this one:

$$Y_{u, i} = \mu + b_i + \epsilon_{u, i}$$

Using the least square,to calculate the $b_i$ that will be the average of $Y_{u, i}$ minus the overall mean ( \textcolor{greenJoe}{\textbf{mu\_adv}}) for each movie i. 

For this estimation, it is use the following code:

```{r Apply least square, echo=TRUE}
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu_adv))
```

These $b_i$ are also refered as the "BIAS".

In the following table is shown the distribution of the BIAS

```{r Output BIAS, echo=FALSE, fig.height=3, fig.width=3, fig.align='center'}
movie_avgs %>%
  ggplot(aes(b_i)) +
  geom_histogram(bins = 10, colour= "blue", fill="light green") +
  labs(title = " DISTRIBUTION OF BIAS") +
  xlab("VALUES") + 
  ylab("NUMBER OF EVALUATIONS") + 
  theme(plot.title = element_text(color = "blue", size = 11, 
                                  face = "bold", hjust = 0.5)) +
  theme(axis.title.x = element_text(color = "grey", size = 10, 
                                    face = "bold", hjust = 0.5)) +
  theme(axis.title.y = element_text(color = "grey", size = 10, 
                                    face = "bold", hjust = 0.5)) +
  theme(axis.text.x = element_text(color = "light blue", size = 10, 
                                   face = "bold", hjust = 0.5)) +
  theme(axis.text.y = element_text(color = "light blue", size = 10, 
                                   face = "bold", hjust = 0.5))
```


These varies substantially is because some movies are good, other average and some bad.

Using these bias there will be calculated the ratings using the following code:

```{r Calculated Rating, echo=TRUE}
movie_model <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  mutate(prediction = mu_adv + b_i) %>%
  pull(prediction)
```

With these new ratings generated b the model, will be calculate the RMSE against the test set

```{r Calculate RMSE, echo=TRUE}
movie_result <- RMSE( movie_model,test_set$rating)
```

These are the results of the enhance model with the "Movies Effect":

```{r Result second Algorithm, echo=TRUE}
movie_result
```

Including the model results in our table of \textcolor{greenJoe}{\textbf{rmse\_results}}

```{r Bind output, include=FALSE}
rmse_results <- bind_rows(rmse_results,
            data.frame(METHOD = "Movie-Based Model", RMSE = movie_result))
```

These is the updated table with the two results:

```{r Show Table, echo=FALSE}
rmse_results %>% knitr::kable()
```


\begin{center}

\large { \textcolor{blueJoe}{\textbf{THIRD MODEL}} }

\end{center}


Another optimization to the model is to consider if the amount of evaluations the users made have any influence in the overall accuracy of the ratings. 

Let's explore these effect for users that have rated more than 100 movies in a chart with their average evaluations.

The following table shows this relationship:

```{r User Eval Chart, echo=FALSE, fig.height=3, fig.width=4, fig.align='center'}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, colour= "blue", fill="light green") +
  labs(title = " AVERAGE RATINGS OF USERS THAT EVALUATED MORE THAN 100 MOVIES") +
  xlab("AVERAGE RATING") + 
  ylab("NUMBER OF EVALUATIONS") + 
  theme(plot.title = element_text(color = "blue", size = 7, 
                                  face = "bold", hjust = 0.5)) +
  theme(axis.title.x = element_text(color = "grey", size = 7, 
                                    face = "bold", hjust = 0.5)) +
  theme(axis.title.y = element_text(color = "grey", size = 7, 
                                    face = "bold", hjust = 0.5)) +
  theme(axis.text.x = element_text(color = "light blue", size = 7, 
                                   face = "bold", hjust = 0.5)) +
  theme(axis.text.y = element_text(color = "light blue", size = 7, 
                                   face = "bold", hjust = 0.5))
```

Now there will be calculate this effect adding the user evaluation in the model.

The following formula will be used adding the effect of the users

$$Y_{u, i} = \mu + b_i + b_u + \epsilon_{u, i}$$

Generating the effect of the users $b_u$ 

The following code calculates the effect of the users:

```{r User adjustment, echo=TRUE}
user_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu_adv - b_i))
```

With this user Adjustment, there will be generate the new adjusted model using the following code:

```{r Predict with user and BIAS, echo=TRUE}
movie_user_model <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu_adv + b_i + b_u) %>%
  pull(pred)
```

With the new estimation it is calculate the RMSE for the model with this code:

```{r RMSE Bias and User, echo=TRUE}
movie_user_result <- RMSE(movie_user_model, test_set$rating)
```

The following is the result of this model that takes now into account the movies as well as the user rating adjustment:

```{r Result 3rd Algorithm, echo=TRUE}
movie_user_result
```

Include the new model results in our table of \textcolor{green}{$\textbf{rmse\_results}$}

```{r Include 3rd result, include=FALSE}
rmse_results <- bind_rows(rmse_results,
                          data.frame(METHOD = "Movie+User Model", 
                          RMSE = movie_user_result))
```

\pagebreak

This is the updated \textcolor{green}{$\textbf{rmse\_results}$} table:

```{r Show table 3, echo=FALSE}
rmse_results %>% knitr::kable()
```

\begin{center}

\large { \textcolor{blueJoe}{\textbf{FOURTH MODEL}} }

\end{center}


Another optimization to the model is to consider if the genres have any influence in the overall accuracy of the ratings. 

We show a table of the top 20 genres and the amount of movies within that genre:

```{r genre table, echo=FALSE, fig.height=3, fig.width=4, fig.align='center'}
train_set %>% 
  group_by(genres) %>%
  summarise(num_movies = n()/1000) %>%
  arrange(desc(num_movies)) %>%
  top_n(20) %>%
  ggplot(aes(x = genres, y= num_movies)) +
  geom_bar( colour= "blue", fill="light green",stat='identity') +
  labs(title = " DISTRIBUTION OF TOP 20 GENRES") +
  xlab("GENRES") + 
  ylab("NUMBER OF MOVIES IN THOUSANDS") + 
  theme(plot.title = element_text(color = "blue", size = 8, 
                                  face = "bold", hjust = 0.5)) +
  theme(axis.title.x = element_text(color = "grey", size = 7, 
                                    face = "bold", hjust = 0.5)) +
  theme(axis.title.y = element_text(color = "grey", size = 7, 
                                    face = "bold", hjust = 0.5)) +
  theme(axis.text.x = element_text(color = "light blue", size = 7, 
                                   face = "bold", hjust = 0.5, 
        angle = 90)) +
  theme(axis.text.y = element_text(color = "light blue", size = 7, 
                                   face = "bold", hjust = 0.5)) 
```

It looks like there is enough potential influence to enhance our model

Let's explore these effect for genres have by adding a $b_g$ to the model

Now the effect of adding the genres evaluation will be calculate.

The following formula will be used adding the effect of the users

$$Y_{u, i} = \mu + b_i + b_u + b_g + \epsilon_{u, i}$$

Generating the effect of the Genres $b_g$ 

The following code calculates the effect of the Genres:

```{r Genres, echo=TRUE}
genre_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by= 'userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu_adv - b_i - b_u))
```

With this user Adjustment there will be generate the new adjusted model using the following code:

```{r Gen, echo=TRUE}
movie_user_genres_model <- test_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  mutate(pred = mu_adv + b_i + b_u + b_g) %>%
  pull(pred)
```

With the new estimation it will calculate the RMSE for the model with this code:

```{r Genre est, echo=TRUE}
movie_user_genres_result <- RMSE(movie_user_genres_model, test_set$rating)
```

The following is the result of this model that takes now into account the movies as well as the user  and the genres in the rating adjustment:

```{r Result 4th, echo=TRUE}
movie_user_genres_result
```

Including the new model results in our table of \textcolor{green}{$\textbf{rmse\_results}$}

```{r Include 4th, include=FALSE}
rmse_results <- bind_rows(rmse_results,
                          data.frame(METHOD = "Movie+User+Genres Model", 
                                     RMSE = movie_user_genres_result))
```

This is the updated \textcolor{green}{$\textbf{rmse\_results}$} table:

```{r Update 4, echo=FALSE}
rmse_results %>% knitr::kable()
```

\begin{center}

\large { \textcolor{blueJoe}{\textbf{FIFTH MODEL}} }

\end{center}


Now applying the concept of Regularization to the 3 elements of our previous model. 

Let's explore these effect Regularization in $b_i$ , $b_u$, $b_g$ to the previous model

The following formula will be used adding the effect of Regularization

$$\frac{1}{N}\sum_{u, i, g}(y_{u, i, g}-\mu-b_i-b_u-b_g)^2 + \lambda(\sum_i b_{i}^2 + \sum_u b_{u}^2 + \sum_g b_{g}^2)$$

The following code calculates the effect Regularization in $b_i$, $b_u$ and $b_g$, it will be calculated with Lambdas from 0 to 10

```{r Best Model, echo=TRUE}
lambdas <- seq(0, 10, 0.25)
# Compute the predicted ratings on test_set dataset using different values of lambda
rmses <- sapply(lambdas, function(lambda) {
  # Calculate the average by movie
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu_adv) / (n() + lambda))
  # Calculate the average by user
  b_u <- train_set %>%
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu_adv) / (n() + lambda))
  # Calculate the average by genre
  b_g <- train_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i - mu_adv - b_u) / (n() + lambda))

  # Compute the predicted ratings on testing data dataset
  predicted_ratings <- test_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres') %>%
    mutate(pred = mu_adv + b_i + b_u + b_g) %>%
    pull(pred)
  
  # Predict the RMSE on the testing set
  return(RMSE(predicted_ratings,test_set$rating))
})

# Get the lambda value that minimize the RMSE
min_lambda <- lambdas[which.min(rmses)]

# Get the "best" minimize RMSE function
reg_movie_user_genres_result <- min(rmses)
reg_movie_user_genres_result
```

Including the new model results in our table of \textcolor{green}{$\textbf{rmse\_results}$}

```{r Write 5th, include=FALSE}
rmse_results <- bind_rows(rmse_results,
                          data.frame(METHOD = "Reg Movie+User+Genres Model", 
                                     RMSE = reg_movie_user_genres_result))
```

This is the updated \textcolor{green}{$\textbf{rmse\_results}$} table:

```{r Show Last Result, echo=FALSE}
rmse_results %>% knitr::kable()
```

\begin{center}

\Large { \textcolor{blue}{Final Validation of the "Best Model:} }

\end{center}

Uploading the \textcolor{blueJoe}{\textbf{validation}} to calculate the actual  precision with "best" model to validate the objective to have < than \textcolor{blueJoe}{\textbf{.86490}} in the RSME function.

```{r Load validation, echo=TRUE, fig.align='center', fig.height=4, fig.width=4}

load(file = "rda/validation.rda")
```

\begin{center}

\large { \textcolor{blueJoe}{\textbf{FINAL BEST MODEL EVALUATION}} }

\end{center}

This is the actual code for the edx set with the validation set as the test set

```{r Execute EDX Verifiaction, echo=TRUE}
lambdas <- seq(0, 10, 0.25)
# Compute the final ratings on validation dataset using different values of lambda
# and the whole edx data set

rmses <- sapply(lambdas, function(lambda) {
  
  # Calculate the average by movie
  b_i <- edx %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu_adv) / (n() + lambda))
  
  # Calculate the average by user
  b_u <- edx %>%
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu_adv) / (n() + lambda))
  
  # Calculate the average by genre
  b_g <- edx %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i - mu_adv - b_u) / (n() + lambda))
  
  # Compute the predicted ratings on validation dataset
  predicted_ratings <- validation %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres') %>%
    mutate(pred = mu_adv + b_i + b_u + b_g) %>%
    pull(pred)
  
  # Predict the RMSE on the validation set
  return(RMSE(predicted_ratings,validation$rating))
})

# Get the lambda value that minimize the RMSE
min_lambda <- lambdas[which.min(rmses)]

# Predict the RMSE on the validation set
min_lambda

final_result <- min(rmses)

# Display the result of the RMSE function
final_result 
```

\begin{center}

\Large { \textcolor{blue}{Conclution:} }

\end{center}

This report show very clearly how we can add more relevant predictors and combined them with a final Regularization effect and  achieve a very reasonable result obtaining the objective of being below the  \textcolor{blueJoe}{\textbf{.86490}} target in the RSME function.

